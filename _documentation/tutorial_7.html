<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Bayesian polynomial regression &#8212;   v8.1 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse and tensor grid quadrature rules" href="tutorial_8.html" />
    <link rel="prev" title="Polynomial regression" href="tutorial_6.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="../_static/logo_white_font.png"></span>
           </a>
        <span class="navbar-text navbar-version pull-left"><b>8.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Explore <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Effective Quadratures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#code">Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#workshops">Workshops</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#papers-theory-and-applications">Papers (theory and applications)</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#get-in-touch">Get in touch</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="team.html">About us</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="basis.html">Basis</a></li>
<li class="toctree-l2"><a class="reference internal" href="poly.html">Polynomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="subspaces.html">Dimension reduction with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimisation.html">Optimisation with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="polynet.html">Deep learning with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="correlated.html">Correlation mapping for polynomials</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1.html">Defining a parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2.html">Generating univariate quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3.html">Constructing orthogonal polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4.html">Computing moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5.html">Multi-index sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6.html">Polynomial regression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Bayesian polynomial regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_8.html">Sparse and tensor grid quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_17.html">Surrogate-Based Optimisation with Polynomials</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="bayesian-polynomial-regression">
<h1>Bayesian polynomial regression<a class="headerlink" href="#bayesian-polynomial-regression" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we present a Bayesian analogue to polynomial regression; the material presented here is heavily derived  from Chapter 3.8 in Rogers and Girolami. We will aim to fit a Bayesian polynomial regression model to the data we considered in the prior tutorial. Bayesian approaches are characterized by four key elements: the model, the prior, the likelihood and the posterior.</p>
<p><strong>The model</strong></p>
<p>We define our model as</p>
<div class="math notranslate nohighlight">
\[\mathbf{f} = \mathbf{Ax} + \boldsymbol{\epsilon},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{N \times M}\)</span> is our <em>Vandermonde</em> type matrix, comprising of <span class="math notranslate nohighlight">\(M\)</span> orthogonal polynomials evaluated at the <span class="math notranslate nohighlight">\(N\)</span> <em>sample inputs</em>. In other words each entry of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> can be given by</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}_{ij} = \psi_i \left(  \lambda_j \right).\]</div>
<p>Returning back to the first equation, we have <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} \in \mathbb{R}^{N}\)</span> which is the noise associated with each <em>sample output</em>, i.e., each element of the vector. To simplify matters, we introduce the standard Gaussian assumption for the noise</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\epsilon} = \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span> indicates the zero vector and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> denotes the covariance matrix associated with the noisy measurements. We assume that these are known. Infact we will assume that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the identity matrix; we set <span class="math notranslate nohighlight">\(\sigma=1.5\)</span>.</p>
<p><strong>The likelihood</strong></p>
<p>In a Bayesian context, the likelihood can be interpreted as a model of some parameters given data. It is formed from the joint probability of both the same data and its associated model parameters. Our likelihood function here is given by</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) = \mathcal{N} \left( \mathbf{Ax}, \sigma^2 \mathbf{I}     \right).\]</div>
<p><strong>The prior</strong></p>
<p>One of the goals of polynomial regression is to ascertain the value of our coefficients <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As we would like an exact expression for the posterior, as Rogers and Girolami state, we require a prior that is <strong>conjugate</strong> to the Gaussian likelihood above. The simplest choice for facilitating this conjugacy is to ensure that our prior too is Gaussian. We therefore write our prior as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right) = \mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right),\]</div>
<p>where we set <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \boldsymbol{0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{0} = 10 \mathbf{I}\)</span>. The high-level idea is that we generate random coefficients—sampling from <span class="math notranslate nohighlight">\(\mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right)\)</span>—for the first six Legendre polynomials and plot them!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">myBasis</span> <span class="o">=</span> <span class="n">Basis</span><span class="p">(</span><span class="s1">&#39;Univariate&#39;</span><span class="p">)</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">myParameters</span><span class="p">,</span> <span class="n">myBasis</span><span class="p">)</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># Extracting the Vandermonde-type matrix!</span>

<span class="c1"># Define the prior!</span>
<span class="n">mu_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Sigma_0</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">Sigma_0</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-center" id="id1">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_a.png"><img alt="../_images/tutorial_7_fig_a.png" src="../_images/tutorial_7_fig_a.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples from the prior distribution (grey lines) assigned to the polynomial coefficients.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p><strong>The posterior</strong></p>
<p>But clearly this isn’t too informative. What we want is to posterior! The posterior can be expressed as a product of the likelihood and the prior. In this tutorial, we know our posterior will also be a Gaussian distribution. What is not known is its precise mean and covariance. Using Bayes’ rule, one can express the posterior distribution as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \mathbf{f}, \mathbf{A}, \sigma^2  \right) \propto  p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right),\]</div>
<p>which upon simplification yields</p>
<div class="math notranslate nohighlight">
\[= \text{exp} \left\{  -\frac{1}{2} \left( \frac{1}{\sigma^2} \left( \mathbf{f} - \mathbf{Ax}  \right)^{T}  \left( \mathbf{f} - \mathbf{Ax}  \right)  +   \left( \mathbf{x} - \boldsymbol{\mu} \right)^{T}  \boldsymbol{\Sigma}_{0}^{-1}  \left( \mathbf{x} - \boldsymbol{\mu} \right)     \right)     \right\}.\]</div>
<p>The covariance and the mean can then be given as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_{\mathbf{x}} = \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{A}  + \boldsymbol{\Sigma}^{-1}_{0}    \right)^{-1}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_{\mathbf{x}} = \boldsymbol{\Sigma}_{\mathbf{x}} \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{f} + \boldsymbol{\Sigma}_{0}^{-1} \boldsymbol{\mu}_{0}   \right)\]</div>
<p>respectively.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span> <span class="c1"># only selecting 4 sample points!</span>
<span class="n">x_use</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">y_use</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">Sigma_measurements</span> <span class="o">=</span> <span class="n">noise</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>
<span class="n">P_data</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_use</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># and now computing the posterior covariance and mean...</span>
<span class="n">Sigma_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span> <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">P_data</span>  <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_0</span><span class="p">)</span> <span class="p">)</span>
<span class="n">mu_x</span> <span class="o">=</span> <span class="n">Sigma_x</span> <span class="o">@</span>   <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">y_use</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Sigma_x</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_b.png"><img alt="../_images/tutorial_7_fig_b.png" src="../_images/tutorial_7_fig_b.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples created from the coefficients drawn from the posterior after observing four data points. The mean is shown in blue.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>The full source code for this tutorial can be found <a class="reference external" href="https://github.com/Effective-Quadratures/Effective-Quadratures/blob/master/source/_documentation/codes/tutorial_7.py">here.</a></p>
<p><strong>References</strong></p>
<ul class="simple">
<li><p>Rogers, S., Girolami, M. (2016). A First Course in Machine Learning. Chapman and Hall/CRC, Boca Raton, Florida, U.S.A.</p></li>
</ul>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016-2019 by Effective Quadratures.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.0.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>