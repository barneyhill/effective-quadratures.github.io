<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Vector-valued dimension reduction &#8212;   v8.1 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep learning via polynomials" href="tutorial_15.html" />
    <link rel="prev" title="Polynomial variable projection" href="tutorial_12.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="../_static/logo_white_font.png"></span>
           </a>
        <span class="navbar-text navbar-version pull-left"><b>8.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Explore <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Effective Quadratures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#code">Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#workshops">Workshops</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#papers-theory-and-applications">Papers (theory and applications)</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#get-in-touch">Get in touch</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="team.html">About us</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="basis.html">Basis</a></li>
<li class="toctree-l2"><a class="reference internal" href="poly.html">Polynomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="subspaces.html">Dimension reduction with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimisation.html">Optimisation with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="polynet.html">Deep learning with polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="correlated.html">Correlation mapping for polynomials</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1.html">Defining a parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2.html">Generating univariate quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3.html">Constructing orthogonal polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4.html">Computing moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4b.html">Multi-index sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5.html">Sparse and tensor grid quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6a.html">Polynomial regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6.html">Polynomial regression for time varying data</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7.html">Polynomial least squares approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_8.html">Polynomials via compressive sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9.html">Computing Sobol’ (sensitivity) indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9b.html">Higher order Sobol’ indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_10.html">Nataf transform for correlated inputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_11.html">Active subspaces with polynomial approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_12.html">Polynomial variable projection</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Vector-valued dimension reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_15.html">Deep learning via polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_16.html">Embedded Ridge Approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_17.html">Surrogate-Based Optimisation with Polynomials</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="vector-valued-dimension-reduction">
<h1>Vector-valued dimension reduction<a class="headerlink" href="#vector-valued-dimension-reduction" title="Permalink to this headline">¶</a></h1>
<p>In some applications, we may be interested in the approximation of vector-valued objectives. This allows us to study the dimension reducing subspaces of multiple scalar objectives, such as the lift and drag of an airfoil simultaneously. Alternatively, we may be interested in the discretized pressure profile over this airfoil. With an approach that extends the technique of <a class="reference external" href="https://effective-quadratures.github.io/Effective-Quadratures/_documentation/tutorial_11.html">active subspaces</a>, and the closely related ridge approximation, we can find subspaces within which the greatest variation of the vector-valued function will be contained. In other words, for a function <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}^n\)</span>, we want to find a ridge approximation</p>
<div class="math notranslate nohighlight">
\[\mathbf{f}(\mathbf{x}) \approx \mathbf{g}( \mathbf{U}^T \mathbf{x}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{U} \in \mathbb{R}^{k \times d}\)</span> with <span class="math notranslate nohighlight">\(k \ll d\)</span>. In Effective Quadratures, we base our approach on Zahm et al. [1] and study the vector gradient covariance matrix. Let us define the Jacobian matrix as</p>
<div class="math notranslate nohighlight">
\[\mathbf{J}(\mathbf{x}) = \left[\frac{\partial f_1}{\partial \mathbf{x}} , \frac{\partial f_2}{\partial \mathbf{x}} ,..., \frac{\partial f_n}{\partial \mathbf{x}} \right],\]</div>
<p>where <span class="math notranslate nohighlight">\(f_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th output component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>. This matrix is constructed by stacking together the gradient vectors <span class="math notranslate nohighlight">\(\frac{\partial f_i}{\partial \mathbf{x}}\)</span>.</p>
<p>So, how are the gradients evaluated? Assuming that automatic differentiation utilities or adjoints are not available, we can adopt an approach similar to that detailed in the <a class="reference external" href="https://effective-quadratures.github.io/Effective-Quadratures/_documentation/tutorial_11.html">active subspaces tutorial</a> , and assume that each <span class="math notranslate nohighlight">\(f_i\)</span> has already been approximated as a polynomial series,</p>
<div class="math notranslate nohighlight">
\[f_i(\mathbf{x}) \approx \sum_{j=1}^P a_j \phi_j(\mathbf{x}),\]</div>
<p>from which gradients are easily evaluated. Then, we can form the gradient covariance matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\mathbf{H} = \int \mathbf{J}(\mathbf{x}) \mathbf{R} \mathbf{J}(\mathbf{x})^T \rho(\mathbf{x}) d\mathbf{x}.\]</div>
<p>In this expression, the positive semi-definite matrix <span class="math notranslate nohighlight">\(\mathbf{R}\)</span> represents the weighting of objectives. We can place more weight on some objectives than others, analogous to a weighted objective function derived from multiple objectives in an optimization problem.</p>
<p>We can then compute the eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> as in the scalar output case, and retain the eigenvectors corresponding to the largest eigenvalues as the columns of the dimension reducing matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>. A Monte-Carlo based strategy for vector-valued dimension reduction using EQ is then</p>
<ol class="arabic simple">
<li><p>For each component <span class="math notranslate nohighlight">\(f_i(\mathbf{x})\)</span> of the function <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>, we fit a polynomial surrogate, <span class="math notranslate nohighlight">\(f_i(\mathbf{x}) \approx p_i(\mathbf{x}) = \sum_{j=1}^P a_j \phi_j(\mathbf{x})\)</span></p></li>
<li><p>Sample each polynomial at <span class="math notranslate nohighlight">\(M\)</span> points drawn from the input distribution <span class="math notranslate nohighlight">\(\rho(\mathbf{x})\)</span> and evaluate the gradient at each point.</p></li>
<li><p>Form the Jacobian matrix using the polynomial surrogates at each input point, <span class="math notranslate nohighlight">\(\mathbf{J}(\mathbf{x}_i) = \left[\frac{\partial p_1}{\partial \mathbf{x}}(\mathbf{x}_i) , \frac{\partial p_2}{\partial \mathbf{x}} (\mathbf{x}_i),..., \frac{\partial p_n}{\partial \mathbf{x}}  (\mathbf{x}_i)\right]\)</span> for <span class="math notranslate nohighlight">\(i = 1,...,M\)</span>.</p></li>
<li><p>Evaluate the vector gradient covariance matrix with a prescribed <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathbf{H} = \frac{1}{M}\sum_{i=1}^M \mathbf{J}(\mathbf{x}_i) \mathbf{R} \mathbf{J}(\mathbf{x}_i)^T\]</div>
<ol class="arabic simple" start="5">
<li><p>Compute the eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> and partition the eigenvectors with large eigenvalues to obtain <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{H} = [\mathbf{U}\quad \mathbf{V}] \begin{bmatrix} \mathbf{\Lambda}_1 &amp; \\ &amp; \mathbf{\Lambda}_2 \end{bmatrix} [\mathbf{U} \quad \mathbf{V}]^T,\end{split}\]</div>
<p><strong>Code Implementation</strong></p>
<p>We consider a simple analytical example to demonstrate the use of our vector-valued dimension reduction routines. Consider the function <span class="math notranslate nohighlight">\(\mathbf{f}: [-1,1]^5 \rightarrow \mathbb{R}^2\)</span>, with <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = [f_0(\mathbf{x}), f_1(\mathbf{x})]^T\)</span> where:</p>
<div class="math notranslate nohighlight">
\[f_0(\mathbf{x}) = \sin(\pi \mathbf{w}_0^T \mathbf{x})\]</div>
<div class="math notranslate nohighlight">
\[f_1(\mathbf{x}) = \exp(\mathbf{w}_1^T \mathbf{x})\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">equadratures</span> <span class="k">import</span> <span class="o">*</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
<p>We define <span class="math notranslate nohighlight">\(\mathbf{W} = [\mathbf{w}_0, \mathbf{w}_1]\)</span> as random orthogonal vectors…</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rand_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">W</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rand_norm</span><span class="p">)</span>
</pre></div>
</div>
<p>and use 1000 samples to construct seventh-degree polynomials for the component functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">poly_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">myBasis</span> <span class="o">=</span> <span class="n">Basis</span><span class="p">(</span><span class="s1">&#39;Total order&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">Parameter</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">Y_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Y_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="n">poly_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Polyreg</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">myBasis</span><span class="p">,</span> <span class="n">training_inputs</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">training_outputs</span><span class="o">=</span><span class="n">Y_train</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">no_of_quad_points</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Now we can call the <code class="code docutils literal notranslate"><span class="pre">vector_AS</span></code> method in the <code class="code docutils literal notranslate"><span class="pre">dr</span></code> class to compute the Jacobian and find the dimension reducing subspace <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_dr</span> <span class="o">=</span> <span class="n">dr</span><span class="p">(</span><span class="n">training_input</span><span class="o">=</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="p">[</span><span class="n">eigs</span><span class="p">,</span> <span class="n">U</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_dr</span><span class="o">.</span><span class="n">vector_AS</span><span class="p">(</span><span class="n">poly_list</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="n">R</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
</pre></div>
</div>
<p>To verify whether our answer is reasonable, we can check the subspace distance between <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and the space spanned by the true dimension reducing subspace, <span class="math notranslate nohighlight">\(\text{span}(\mathbf{w}_0, \mathbf{w}_1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{dist}(\mathbf{U}, \mathbf{W}) = ||\mathbf{U}\mathbf{U}^T - \mathbf{W}\mathbf{W}^T||_2\]</div>
<p>where the 2-norm picks out the largest singular value of the argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">subspace_dist</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">subspace_dist</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,:</span><span class="n">n</span><span class="p">])))</span>
</pre></div>
</div>
<p>We can also verify that the variation of both functions outside of <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is small. Here, for each of 10 sets of active coordinates, we sample 50 points in the inactive subspace (<span class="math notranslate nohighlight">\(\mathbf{V}\)</span>) using the “hit and run” algorithm from Python Active-subspaces Utility Library [2].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">active_subspaces.domains</span> <span class="k">import</span> <span class="n">hit_and_run_z</span>
<span class="k">def</span> <span class="nf">harz</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span><span class="n">W2</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">N</span><span class="p">):</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span><span class="n">W2</span><span class="p">])</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">hit_and_run_z</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
        <span class="n">yz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">N</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">yz</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">N_inactive</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">plot_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N_inactive</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">rand_active_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">new_X</span> <span class="o">=</span> <span class="n">harz</span><span class="p">(</span><span class="n">U</span><span class="p">[:,:</span><span class="n">n</span><span class="p">],</span> <span class="n">U</span><span class="p">[:,</span><span class="n">n</span><span class="p">:],</span> <span class="n">rand_active_coords</span><span class="p">,</span> <span class="n">N_inactive</span><span class="p">)</span>

        <span class="n">new_f0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">new_X</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">new_f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">new_X</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">plot_coords</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_f0</span>
        <span class="n">plot_coords</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_f1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">plot_coords</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">plot_coords</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/inactive.png"><img alt="../_images/inactive.png" src="../_images/inactive.png" style="width: 640.0px; height: 480.0px;" /></a>
</div>
<p>It can be seen that at each active coordinate, the variation in the inactive subspace is small.</p>
<p><strong>References</strong></p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Zahm, O.,  Constantine, P., Prieur, C. and Marzouk, Y., (2018). Gradient-based dimension reduction of multi-variate vector-valued functions. <a class="reference external" href="http://arxiv.org/abs/1801.07922">Preprint</a></p>
</dd>
<dt class="label" id="id2"><span class="brackets">2</span></dt>
<dd><p>Constantine, P., Howard, R., Glaws, A., Grey, Z., Diaz, P., &amp; Fletcher, L. (2016). Python Active-subspaces Utility Library. Zenodo. <a class="reference external" href="http://doi.org/10.5281/zenodo.158941">Code</a></p>
</dd>
</dl>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016-2019 by Effective Quadratures.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.0.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>